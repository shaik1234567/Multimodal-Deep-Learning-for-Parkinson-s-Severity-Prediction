{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",

      ],
      "metadata": {
        "id": "z-RpkH9LWDS7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghg8FIOuZ6Vv",
        "outputId": "4b210849-4fb4-4d9b-af20-c1d65073317d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "pip install pandas numpy matplotlib torch torchvision scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df[FEATURE_COLS] = df[FEATURE_COLS].astype(float)\n"
      ],
      "metadata": {
        "id": "EqEDaToWax7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Normalize features (simple scaling for demo)\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# scaler = StandardScaler()\n",
        "# df[FEATURE_COLS] = scaler.fit_transform(df[FEATURE_COLS])\n",
        "\n",
        "# # Force feature columns to float type (important!)\n",
        "# df[FEATURE_COLS] = df[FEATURE_COLS].astype(float)\n"
      ],
      "metadata": {
        "id": "9Y14IZOga02n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Force convert to numeric, coercing errors to NaN\n",
        "# for col in FEATURE_COLS:\n",
        "#     df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "# # Check if any NaN appeared (means some conversion failed)\n",
        "# print(df[FEATURE_COLS].isna().sum())\n",
        "\n",
        "# # If there are NaNs, fill them (e.g., with 0 or column mean)\n",
        "# df[FEATURE_COLS] = df[FEATURE_COLS].fillna(0)  # or df.fillna(df.mean())\n"
      ],
      "metadata": {
        "id": "HUThjb73bJXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df[FEATURE_COLS] = df[FEATURE_COLS].astype(float)\n"
      ],
      "metadata": {
        "id": "xevFgfmebPP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(df[FEATURE_COLS].dtypes)\n"
      ],
      "metadata": {
        "id": "les0i2-TbQjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def __getitem__(self, idx):\n",
        "    row = self.df.iloc[idx]\n",
        "    tabular_features = torch.tensor(row[self.feature_cols].to_numpy(dtype=np.float32), dtype=torch.float32)\n",
        "    target = torch.tensor(float(row[self.target_col]), dtype=torch.float32)\n",
        "\n",
        "    # Load image as before (unchanged)\n",
        "    img_path = row[\"img_path\"]\n",
        "    image = Image.open(img_path).convert('L')\n",
        "    if self.transform:\n",
        "        image = self.transform(image)\n",
        "\n",
        "    return tabular_features, image, target\n"
      ],
      "metadata": {
        "id": "8grzHHyFbzBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample data - replace this with your actual DataFrame loading\n",
        "data = {\n",
        "    \"subject#\": [1, 1],\n",
        "    \"age\": [72, 72],\n",
        "    \"sex\": [0, 0],\n",
        "    \"test_time\": [5.6431, 12.666],\n",
        "    \"motor_UPDRS\": [28.199, 28.447],\n",
        "    \"total_UPDRS\": [34.398, 34.894],\n",
        "    \"Jitter(%)\": [0.00662, 0.003],\n",
        "    \"Jitter(Abs)\": [3.38e-5, 1.68e-5],\n",
        "    \"Jitter:RAP\": [0.00401, 0.00132],\n",
        "    \"Jitter:PPQ5\": [0.00317, 0.0015],\n",
        "    \"Jitter:DDP\": [0.01204, 0.00395],\n",
        "    \"Shimmer\": [0.02565, 0.02024],\n",
        "    \"Shimmer(dB)\": [0.23, 0.179],\n",
        "    \"Shimmer:APQ3\": [0.01438, 0.00994],\n",
        "    \"Shimmer:APQ5\": [0.01309, 0.01072],\n",
        "    \"Shimmer:APQ11\": [0.01662, 0.01689],\n",
        "    \"Shimmer:DDA\": [0.04314, 0.02982],\n",
        "    \"NHR\": [0.01429, 0.011112],\n",
        "    \"HNR\": [21.64, 27.183],\n",
        "    \"RPDE\": [0.41888, 0.43493],\n",
        "    \"DFA\": [0.54842, 0.56477],\n",
        "    \"PPE\": [0.16006, 0.1081],\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Normalize features (example: MinMax scaling)\n",
        "FEATURE_COLS = [\n",
        "    \"age\", \"sex\", \"test_time\", \"Jitter(%)\", \"Jitter(Abs)\", \"Jitter:RAP\", \"Jitter:PPQ5\", \"Jitter:DDP\",\n",
        "    \"Shimmer\", \"Shimmer(dB)\", \"Shimmer:APQ3\", \"Shimmer:APQ5\", \"Shimmer:APQ11\", \"Shimmer:DDA\",\n",
        "    \"NHR\", \"HNR\", \"RPDE\", \"DFA\", \"PPE\"\n",
        "]\n",
        "\n",
        "# Simple Min-Max scaling for example\n",
        "df[FEATURE_COLS] = (df[FEATURE_COLS] - df[FEATURE_COLS].min()) / (df[FEATURE_COLS].max() - df[FEATURE_COLS].min())\n",
        "\n",
        "# Create dummy spectrogram images from tabular data by plotting each row's features as a bar plot saved as an image\n",
        "def create_spectrogram_image(row, save_path):\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.bar(range(len(FEATURE_COLS)), row[FEATURE_COLS])\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path)\n",
        "    plt.close()\n",
        "\n",
        "# Generate images and add file paths to df\n",
        "image_paths = []\n",
        "for i, row in df.iterrows():\n",
        "    img_path = f\"spectrogram_{i}.png\"\n",
        "    create_spectrogram_image(row, img_path)\n",
        "    image_paths.append(img_path)\n",
        "df[\"img_path\"] = image_paths\n",
        "\n",
        "# Dataset class\n",
        "class ParkinsonDataset(Dataset):\n",
        "    def __init__(self, df, feature_cols, target_col, transform=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.feature_cols = feature_cols\n",
        "        self.target_col = target_col\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        tabular_features = torch.tensor(row[self.feature_cols].to_numpy(dtype=np.float32), dtype=torch.float32)\n",
        "        target = torch.tensor(float(row[self.target_col]), dtype=torch.float32)\n",
        "\n",
        "        image = Image.open(row[\"img_path\"]).convert('L')  # grayscale image\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return tabular_features, image, target\n",
        "\n",
        "# Define transforms for images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = ParkinsonDataset(df=df, feature_cols=FEATURE_COLS, target_col=\"total_UPDRS\", transform=transform)\n",
        "loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "# Test the dataloader\n",
        "for tab_data, img, target in loader:\n",
        "    print(\"Tabular features shape:\", tab_data.shape)  # [batch_size, num_features]\n",
        "    print(\"Image shape:\", img.shape)                   # [batch_size, 1, 64, 64]\n",
        "    print(\"Target shape:\", target.shape)               # [batch_size]\n",
        "    break  # Just one batch for test\n"
      ],
      "metadata": {
        "id": "b81X_fpVaAHd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99a2df6f-3b22-4a22-e269-15517b6aca38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tabular features shape: torch.Size([2, 19])\n",
            "Image shape: torch.Size([2, 1, 64, 64])\n",
            "Target shape: torch.Size([2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[FEATURE_COLS].dtypes)\n",
        "print(df[FEATURE_COLS].head())\n"
      ],
      "metadata": {
        "id": "j3m2MikoaDRw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efafd1fb-f56a-4967-e8b2-26230d5f3735"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "age              float64\n",
            "sex              float64\n",
            "test_time        float64\n",
            "Jitter(%)        float64\n",
            "Jitter(Abs)      float64\n",
            "Jitter:RAP       float64\n",
            "Jitter:PPQ5      float64\n",
            "Jitter:DDP       float64\n",
            "Shimmer          float64\n",
            "Shimmer(dB)      float64\n",
            "Shimmer:APQ3     float64\n",
            "Shimmer:APQ5     float64\n",
            "Shimmer:APQ11    float64\n",
            "Shimmer:DDA      float64\n",
            "NHR              float64\n",
            "HNR              float64\n",
            "RPDE             float64\n",
            "DFA              float64\n",
            "PPE              float64\n",
            "dtype: object\n",
            "   age  sex  test_time  Jitter(%)  Jitter(Abs)  Jitter:RAP  Jitter:PPQ5  \\\n",
            "0  NaN  NaN        0.0        1.0          1.0         1.0          1.0   \n",
            "1  NaN  NaN        1.0        0.0          0.0         0.0          0.0   \n",
            "\n",
            "   Jitter:DDP  Shimmer  Shimmer(dB)  Shimmer:APQ3  Shimmer:APQ5  \\\n",
            "0         1.0      1.0          1.0           1.0           1.0   \n",
            "1         0.0      0.0          0.0           0.0           0.0   \n",
            "\n",
            "   Shimmer:APQ11  Shimmer:DDA  NHR  HNR  RPDE  DFA  PPE  \n",
            "0            0.0          1.0  1.0  0.0   0.0  0.0  1.0  \n",
            "1            1.0          0.0  0.0  1.0   1.0  1.0  0.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# === STEP 1: Load your original dataset ===\n",
        "df = pd.read_csv(\"parkinsons_updrs.csv\")\n",
        "\n",
        "# === STEP 2: Define the features you want to visualize ===\n",
        "FEATURE_COLS = [\n",
        "    \"age\", \"sex\", \"test_time\",\n",
        "    \"Jitter(%)\", \"Jitter(Abs)\", \"Jitter:RAP\", \"Jitter:PPQ5\", \"Jitter:DDP\",\n",
        "    \"Shimmer\", \"Shimmer(dB)\", \"Shimmer:APQ3\", \"Shimmer:APQ5\", \"Shimmer:APQ11\", \"Shimmer:DDA\",\n",
        "    \"NHR\", \"HNR\", \"RPDE\", \"DFA\", \"PPE\"\n",
        "]\n",
        "\n",
        "# Optional: Normalize feature columns\n",
        "df[FEATURE_COLS] = (df[FEATURE_COLS] - df[FEATURE_COLS].min()) / (df[FEATURE_COLS].max() - df[FEATURE_COLS].min())\n",
        "\n",
        "# === STEP 3: Create a folder to store the images ===\n",
        "os.makedirs(\"updrs_images\", exist_ok=True)\n",
        "\n",
        "# === STEP 4: Generate bar chart images from the features ===\n",
        "image_paths = []\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    img_filename = f\"updrs_{idx:04}.png\"  # e.g., updrs_0001.png\n",
        "    img_path = os.path.join(\"updrs_images\", img_filename)\n",
        "\n",
        "    # Create the bar chart from features\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.bar(range(len(FEATURE_COLS)), row[FEATURE_COLS])\n",
        "    plt.xticks([])  # Hide x-axis ticks\n",
        "    plt.yticks([])\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(img_path)\n",
        "    plt.close()\n",
        "\n",
        "    image_paths.append(img_path)\n",
        "\n",
        "# === STEP 5: Save the image paths to a new CSV ===\n",
        "image_paths_df = pd.DataFrame({\"image_path\": image_paths})\n",
        "image_paths_df.to_csv(\"updrs_image_paths.csv\", index=False)\n",
        "print(\"✅ Saved image paths as: updrs_image_paths.csv\")"
      ],
      "metadata": {
        "id": "34UHxm3ya8eR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0d6b59d-eb4e-4273-e2ad-cc6114d12dde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved image paths as: updrs_image_paths.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MultiModalNet(nn.Module):\n",
        "    def __init__(self, num_tab_features):\n",
        "        super(MultiModalNet, self).__init__()\n",
        "\n",
        "        # ---- Tabular branch (MLP) ----\n",
        "        self.tabular_net = nn.Sequential(\n",
        "            nn.Linear(num_tab_features, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # ---- Image branch (CNN) ----\n",
        "        self.cnn_net = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),  # input is grayscale (1 channel)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),  # 64 -> 32\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),  # 32 -> 16\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "        # Calculate flattened CNN output size (for 64x64 input)\n",
        "        self._to_linear = 32 * 16 * 16  # 32 channels * 16 * 16\n",
        "\n",
        "        # ---- Fusion + Final layers ----\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self._to_linear + 64, 64),  # concat MLP(64) + CNN output\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)  # regression for UPDRS (use nn.Sigmoid() for binary classification)\n",
        "        )\n",
        "\n",
        "    def forward(self, tabular_data, images):\n",
        "        x_tab = self.tabular_net(tabular_data)           # [batch, 64]\n",
        "        x_img = self.cnn_net(images)                     # [batch, flattened_cnn]\n",
        "        x = torch.cat([x_tab, x_img], dim=1)             # concat features\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "gaJ5y86Fnq34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MultiModalNet(nn.Module):\n",
        "    def __init__(self, num_tab_features):\n",
        "        super(MultiModalNet, self).__init__()\n",
        "\n",
        "        # ---- Tabular branch (MLP) ----\n",
        "        self.tabular_net = nn.Sequential(\n",
        "            nn.Linear(num_tab_features, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # ---- Image branch (CNN) ----\n",
        "        self.cnn_net = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),  # 64 -> 32\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),  # 32 -> 16\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "        # Calculate flattened CNN output size (for 64x64 input)\n",
        "        self._to_linear = 32 * 16 * 16\n",
        "\n",
        "        # ---- Fusion + Final layers ----\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self._to_linear + 64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)  # regression output\n",
        "        )\n",
        "\n",
        "    def forward(self, tabular_data, images):\n",
        "        x_tab = self.tabular_net(tabular_data)   # [batch, 64]\n",
        "        x_img = self.cnn_net(images)             # [batch, flattened_cnn]\n",
        "        x = torch.cat([x_tab, x_img], dim=1)     # concatenate\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Hcp2oRMD6nVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = MultiModalNet(num_tab_features=len(FEATURE_COLS)).to(device)\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVJeXIhc6rLz",
        "outputId": "7839d9d9-58f7-4c75-c16d-9c84b0f85656"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultiModalNet(\n",
            "  (tabular_net): Sequential(\n",
            "    (0): Linear(in_features=19, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.2, inplace=False)\n",
            "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (4): ReLU()\n",
            "  )\n",
            "  (cnn_net): Sequential(\n",
            "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): ReLU()\n",
            "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): Flatten(start_dim=1, end_dim=-1)\n",
            "  )\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=8256, out_features=64, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=64, out_features=1, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = MultiModalNet(num_tab_features=len(FEATURE_COLS)).to(device)\n",
        "criterion = nn.MSELoss()  # regression task\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for tab_data, img, target in loader:\n",
        "        tab_data, img, target = tab_data.to(device), img.to(device), target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(tab_data, img).squeeze()\n",
        "        loss = criterion(outputs, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {running_loss/len(loader):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBzsRAmP5dH5",
        "outputId": "c6563124-acde-4db3-c030-b76cdeb56fb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: nan\n",
            "Epoch [2/10], Loss: nan\n",
            "Epoch [3/10], Loss: nan\n",
            "Epoch [4/10], Loss: nan\n",
            "Epoch [5/10], Loss: nan\n",
            "Epoch [6/10], Loss: nan\n",
            "Epoch [7/10], Loss: nan\n",
            "Epoch [8/10], Loss: nan\n",
            "Epoch [9/10], Loss: nan\n",
            "Epoch [10/10], Loss: nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rikn7ghH5gE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9c07eff"
      },
      "source": [
        "# Task\n",
        "Modify the provided code to train a multimodal model for Parkinson's disease detection using both tabular voice signal data and image representations of the same data. The image paths should be loaded from the \"updrs_image_paths.csv\" file. The model should be trained and validated, and the final performance should be evaluated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25af66e2"
      },
      "source": [
        "## Update dataset loading\n",
        "\n",
        "### Subtask:\n",
        "Modify the `ParkinsonDataset` class to load image paths from the `updrs_image_paths.csv` file and combine the tabular data with the image paths.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39aafa63"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the image paths and merge them with the main dataframe, then redefine the Dataset class to use the merged dataframe.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c1c2788",
        "outputId": "bb991429-cf54-4d2f-de4d-c71f888eadfe"
      },
      "source": [
        "# Load image paths\n",
        "image_paths_df = pd.read_csv(\"updrs_image_paths.csv\")\n",
        "\n",
        "# Merge with the main DataFrame (assuming the order of rows is the same)\n",
        "df = df.reset_index(drop=True).merge(image_paths_df.reset_index(drop=True), left_index=True, right_index=True)\n",
        "\n",
        "# Dataset class\n",
        "class ParkinsonDataset(Dataset):\n",
        "    def __init__(self, df, feature_cols, target_col, transform=None):\n",
        "        self.df = df\n",
        "        self.feature_cols = feature_cols\n",
        "        self.target_col = target_col\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        tabular_features = torch.tensor(row[self.feature_cols].to_numpy(dtype=np.float32), dtype=torch.float32)\n",
        "        target = torch.tensor(float(row[self.target_col]), dtype=torch.float32)\n",
        "\n",
        "        image = Image.open(row[\"image_path\"]).convert('L')  # grayscale image\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return tabular_features, image, target\n",
        "\n",
        "# Define transforms for images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = ParkinsonDataset(df=df, feature_cols=FEATURE_COLS, target_col=\"total_UPDRS\", transform=transform)\n",
        "loader = DataLoader(dataset, batch_size=32, shuffle=True) # Increased batch size\n",
        "\n",
        "# Test the dataloader\n",
        "for tab_data, img, target in loader:\n",
        "    print(\"Tabular features shape:\", tab_data.shape)  # [batch_size, num_features]\n",
        "    print(\"Image shape:\", img.shape)                   # [batch_size, 1, 64, 64]\n",
        "    print(\"Target shape:\", target.shape)               # [batch_size]\n",
        "    break  # Just one batch for test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tabular features shape: torch.Size([32, 19])\n",
            "Image shape: torch.Size([32, 1, 64, 64])\n",
            "Target shape: torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6457aba9"
      },
      "source": [
        "## Split data\n",
        "\n",
        "### Subtask:\n",
        "Split the combined data into training and validation sets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a89ed750"
      },
      "source": [
        "**Reasoning**:\n",
        "Split the dataframe into training and validation sets and create corresponding datasets and dataloaders.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d83828ae",
        "outputId": "1e005e2d-eef9-4b37-c441-134e5862d76c"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "# Using a simple random split as stratification by subject# might be complex due to varying numbers of records per subject\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create separate Dataset instances for training and validation\n",
        "train_dataset = ParkinsonDataset(df=train_df, feature_cols=FEATURE_COLS, target_col=\"total_UPDRS\", transform=transform)\n",
        "val_dataset = ParkinsonDataset(df=val_df, feature_cols=FEATURE_COLS, target_col=\"total_UPDRS\", transform=transform)\n",
        "\n",
        "# Create separate DataLoader instances\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"Training data size: {len(train_dataset)}\")\n",
        "print(f\"Validation data size: {len(val_dataset)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data size: 4700\n",
            "Validation data size: 1175\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d008989"
      },
      "source": [
        "## Refine model fusion (optional)\n",
        "\n",
        "### Subtask:\n",
        "Explore alternative methods for fusing tabular and image features in the `MultiModalNet` to potentially improve accuracy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43b99f8c"
      },
      "source": [
        "**Reasoning**:\n",
        "Review the current model definition and identify the fusion layer. Since the current fusion method is simple concatenation followed by linear layers, I will implement an alternative by adding more layers and potentially adjusting activation functions or dropout rates in the fusion part of the network.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70a8cf41"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MultiModalNet(nn.Module):\n",
        "    def __init__(self, num_tab_features):\n",
        "        super(MultiModalNet, self).__init__()\n",
        "\n",
        "        # ---- Tabular branch (MLP) ----\n",
        "        self.tabular_net = nn.Sequential(\n",
        "            nn.Linear(num_tab_features, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # ---- Image branch (CNN) ----\n",
        "        self.cnn_net = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),  # 64 -> 32\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),  # 32 -> 16\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "        # Calculate flattened CNN output size (for 64x64 input)\n",
        "        # Assuming input image size is 64x64 and kernel size 3, padding 1, stride 1 for conv\n",
        "        # After 2 MaxPool2d layers with kernel size 2, the spatial dimensions become 64/2/2 = 16\n",
        "        # Number of channels is 32 after the second convolution\n",
        "        self._to_linear = 32 * 16 * 16\n",
        "\n",
        "        # ---- Fusion + Final layers (Alternative 1: More layers in fusion) ----\n",
        "        self.fusion_net = nn.Sequential(\n",
        "            nn.Linear(self._to_linear + 64, 128), # concat MLP(64) + CNN output\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3), # Increased dropout\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)  # regression for UPDRS\n",
        "        )\n",
        "\n",
        "    def forward(self, tabular_data, images):\n",
        "        x_tab = self.tabular_net(tabular_data)   # [batch, 64]\n",
        "        x_img = self.cnn_net(images)             # [batch, flattened_cnn]\n",
        "        x = torch.cat([x_tab, x_img], dim=1)     # concatenate\n",
        "        x = self.fusion_net(x)                   # Pass through fusion layers\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4774f6d2"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step defined the modified model architecture. This step will initialize the model and print its structure to confirm the changes before proceeding.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "064cc3e3",
        "outputId": "9e8f5488-b0ec-4254-d789-cf55dad82e84"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = MultiModalNet(num_tab_features=len(FEATURE_COLS)).to(device)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultiModalNet(\n",
            "  (tabular_net): Sequential(\n",
            "    (0): Linear(in_features=19, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.2, inplace=False)\n",
            "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (4): ReLU()\n",
            "  )\n",
            "  (cnn_net): Sequential(\n",
            "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): ReLU()\n",
            "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): Flatten(start_dim=1, end_dim=-1)\n",
            "  )\n",
            "  (fusion_net): Sequential(\n",
            "    (0): Linear(in_features=8256, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.3, inplace=False)\n",
            "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Linear(in_features=64, out_features=1, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e9aff41"
      },
      "source": [
        "## Implement training with validation\n",
        "\n",
        "### Subtask:\n",
        "Set up a training loop that includes evaluating the model on the validation set after each epoch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efcc33c2"
      },
      "source": [
        "**Reasoning**:\n",
        "Set up the training and validation loop as described in the instructions, including defining optimizer, loss function, number of epochs, iterating through dataloaders, moving data to device, forward pass, calculating loss, backpropagation, optimizer step, setting model to train/eval mode, and printing epoch losses.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78302b7f",
        "outputId": "ed36a48f-779a-476a-8a4b-f3d31614e1bd"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = MultiModalNet(num_tab_features=len(FEATURE_COLS)).to(device)\n",
        "criterion = nn.MSELoss()  # regression task\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4) # Lowered learning rate\n",
        "\n",
        "EPOCHS = 20 # Increased number of epochs\n",
        "\n",
        "print(f\"Starting training for {EPOCHS} epochs on {device}\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # --- Training ---\n",
        "    model.train()\n",
        "    running_train_loss = 0.0\n",
        "\n",
        "    for tab_data, img, target in train_loader:\n",
        "        tab_data, img, target = tab_data.to(device), img.to(device), target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(tab_data, img).squeeze() # Use squeeze to match target shape if batch size is 1\n",
        "        loss = criterion(outputs, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_train_loss += loss.item() * tab_data.size(0) # Accumulate loss weighted by batch size\n",
        "\n",
        "    epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
        "\n",
        "\n",
        "    # --- Validation ---\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad(): # Disable gradient calculation for validation\n",
        "        for tab_data, img, target in val_loader:\n",
        "            tab_data, img, target = tab_data.to(device), img.to(device), target.to(device)\n",
        "\n",
        "            outputs = model(tab_data, img).squeeze() # Use squeeze to match target shape\n",
        "            loss = criterion(outputs, target)\n",
        "\n",
        "            running_val_loss += loss.item() * tab_data.size(0) # Accumulate loss weighted by batch size\n",
        "\n",
        "    epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{EPOCHS}], Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}\")\n",
        "\n",
        "print(\"Training finished.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 20 epochs on cpu\n",
            "Epoch [1/20], Train Loss: 359.7478, Val Loss: 112.2349\n",
            "Epoch [2/20], Train Loss: 128.4174, Val Loss: 111.1432\n",
            "Epoch [3/20], Train Loss: 126.1224, Val Loss: 110.6453\n",
            "Epoch [4/20], Train Loss: 124.3848, Val Loss: 110.8454\n",
            "Epoch [5/20], Train Loss: 122.8745, Val Loss: 108.5637\n",
            "Epoch [6/20], Train Loss: 121.1134, Val Loss: 106.9864\n",
            "Epoch [7/20], Train Loss: 118.5891, Val Loss: 107.5622\n",
            "Epoch [8/20], Train Loss: 117.1484, Val Loss: 103.9260\n",
            "Epoch [9/20], Train Loss: 114.9292, Val Loss: 104.4400\n",
            "Epoch [10/20], Train Loss: 115.1555, Val Loss: 99.9496\n",
            "Epoch [11/20], Train Loss: 113.2359, Val Loss: 102.7025\n",
            "Epoch [12/20], Train Loss: 110.0858, Val Loss: 97.9129\n",
            "Epoch [13/20], Train Loss: 109.4951, Val Loss: 97.8845\n",
            "Epoch [14/20], Train Loss: 110.0220, Val Loss: 95.3024\n",
            "Epoch [15/20], Train Loss: 107.0798, Val Loss: 96.2480\n",
            "Epoch [16/20], Train Loss: 107.2916, Val Loss: 98.8302\n",
            "Epoch [17/20], Train Loss: 104.4832, Val Loss: 95.5893\n",
            "Epoch [18/20], Train Loss: 105.4136, Val Loss: 94.3358\n",
            "Epoch [19/20], Train Loss: 104.9674, Val Loss: 93.0842\n",
            "Epoch [20/20], Train Loss: 104.6777, Val Loss: 91.8757\n",
            "Training finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b537127c"
      },
      "source": [
        "## Hyperparameter tuning\n",
        "\n",
        "### Subtask:\n",
        "Experiment with different hyperparameters to optimize model performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eebd32c9"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the training loop to accept hyperparameters and run training with different combinations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf59e895",
        "outputId": "d57993e3-20ca-4273-e960-e523821874d6"
      },
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs, device):\n",
        "    \"\"\"Trains and validates the model.\"\"\"\n",
        "    history = {'train_loss': [], 'val_loss': []}\n",
        "\n",
        "    print(f\"Starting training for {epochs} epochs on {device}\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # --- Training ---\n",
        "        model.train()\n",
        "        running_train_loss = 0.0\n",
        "\n",
        "        for tab_data, img, target in train_loader:\n",
        "            tab_data, img, target = tab_data.to(device), img.to(device), target.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(tab_data, img).squeeze()\n",
        "            loss = criterion(outputs, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_train_loss += loss.item() * tab_data.size(0)\n",
        "\n",
        "        epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
        "        history['train_loss'].append(epoch_train_loss)\n",
        "\n",
        "        # --- Validation ---\n",
        "        model.eval()\n",
        "        running_val_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for tab_data, img, target in val_loader:\n",
        "                tab_data, img, target = tab_data.to(device), img.to(device), target.to(device)\n",
        "\n",
        "                outputs = model(tab_data, img).squeeze()\n",
        "                loss = criterion(outputs, target)\n",
        "\n",
        "                running_val_loss += loss.item() * tab_data.size(0)\n",
        "\n",
        "        epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
        "        history['val_loss'].append(epoch_val_loss)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}\")\n",
        "\n",
        "    print(\"Training finished.\")\n",
        "    return history\n",
        "\n",
        "# Define hyperparameter combinations to experiment with\n",
        "hyperparameter_combinations = [\n",
        "    {'lr': 1e-4, 'batch_size': 32, 'epochs': 15},\n",
        "    {'lr': 5e-4, 'batch_size': 64, 'epochs': 15},\n",
        "    {'lr': 1e-3, 'batch_size': 32, 'epochs': 10} # Fewer epochs for a higher learning rate\n",
        "]\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "results = {}\n",
        "\n",
        "for i, combo in enumerate(hyperparameter_combinations):\n",
        "    print(f\"\\n--- Experiment {i+1}/{len(hyperparameter_combinations)} with {combo} ---\")\n",
        "\n",
        "    # Create new model and optimizer for each experiment\n",
        "    model = MultiModalNet(num_tab_features=len(FEATURE_COLS)).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=combo['lr'])\n",
        "\n",
        "    # Create new data loaders with the specified batch size\n",
        "    train_loader = DataLoader(train_dataset, batch_size=combo['batch_size'], shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=combo['batch_size'], shuffle=False)\n",
        "\n",
        "    # Train the model and store history\n",
        "    history = train_model(model, train_loader, val_loader, criterion, optimizer, combo['epochs'], device)\n",
        "    results[f\"Experiment_{i+1}_{str(combo)}\"] = history\n",
        "\n",
        "# Optional: Plotting the results (requires matplotlib)\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# for name, history in results.items():\n",
        "#     plt.figure(figsize=(10, 6))\n",
        "#     plt.plot(history['train_loss'], label='Train Loss')\n",
        "#     plt.plot(history['val_loss'], label='Validation Loss')\n",
        "#     plt.title(f'Loss during training - {name}')\n",
        "#     plt.xlabel('Epoch')\n",
        "#     plt.ylabel('Loss (MSE)')\n",
        "#     plt.legend()\n",
        "#     plt.grid(True)\n",
        "#     plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Experiment 1/3 with {'lr': 0.0001, 'batch_size': 32, 'epochs': 15} ---\n",
            "Starting training for 15 epochs on cpu\n",
            "Epoch [1/15], Train Loss: 367.4629, Val Loss: 111.7254\n",
            "Epoch [2/15], Train Loss: 125.6268, Val Loss: 111.3017\n",
            "Epoch [3/15], Train Loss: 123.7570, Val Loss: 111.2583\n",
            "Epoch [4/15], Train Loss: 124.1724, Val Loss: 113.6024\n",
            "Epoch [5/15], Train Loss: 122.6325, Val Loss: 108.6524\n",
            "Epoch [6/15], Train Loss: 120.6060, Val Loss: 105.4462\n",
            "Epoch [7/15], Train Loss: 119.1375, Val Loss: 103.0686\n",
            "Epoch [8/15], Train Loss: 115.0997, Val Loss: 103.5105\n",
            "Epoch [9/15], Train Loss: 112.5518, Val Loss: 106.0479\n",
            "Epoch [10/15], Train Loss: 111.0528, Val Loss: 97.2155\n",
            "Epoch [11/15], Train Loss: 110.2661, Val Loss: 96.0191\n",
            "Epoch [12/15], Train Loss: 107.2404, Val Loss: 95.5125\n",
            "Epoch [13/15], Train Loss: 105.7719, Val Loss: 98.3098\n",
            "Epoch [14/15], Train Loss: 104.0356, Val Loss: 95.2996\n",
            "Epoch [15/15], Train Loss: 104.8893, Val Loss: 92.7366\n",
            "Training finished.\n",
            "\n",
            "--- Experiment 2/3 with {'lr': 0.0005, 'batch_size': 64, 'epochs': 15} ---\n",
            "Starting training for 15 epochs on cpu\n",
            "Epoch [1/15], Train Loss: 271.9666, Val Loss: 112.7973\n",
            "Epoch [2/15], Train Loss: 128.6598, Val Loss: 111.2473\n",
            "Epoch [3/15], Train Loss: 127.0358, Val Loss: 110.7825\n",
            "Epoch [4/15], Train Loss: 120.3174, Val Loss: 103.2398\n",
            "Epoch [5/15], Train Loss: 116.0606, Val Loss: 108.7629\n",
            "Epoch [6/15], Train Loss: 112.6946, Val Loss: 117.1935\n",
            "Epoch [7/15], Train Loss: 108.7122, Val Loss: 98.2810\n",
            "Epoch [8/15], Train Loss: 108.9034, Val Loss: 100.8988\n",
            "Epoch [9/15], Train Loss: 104.3316, Val Loss: 103.6322\n",
            "Epoch [10/15], Train Loss: 105.4028, Val Loss: 130.5312\n",
            "Epoch [11/15], Train Loss: 101.9070, Val Loss: 112.3436\n",
            "Epoch [12/15], Train Loss: 103.1800, Val Loss: 123.2148\n",
            "Epoch [13/15], Train Loss: 100.5591, Val Loss: 112.9573\n",
            "Epoch [14/15], Train Loss: 98.1951, Val Loss: 107.8033\n",
            "Epoch [15/15], Train Loss: 98.6848, Val Loss: 106.4846\n",
            "Training finished.\n",
            "\n",
            "--- Experiment 3/3 with {'lr': 0.001, 'batch_size': 32, 'epochs': 10} ---\n",
            "Starting training for 10 epochs on cpu\n",
            "Epoch [1/10], Train Loss: 182.3939, Val Loss: 121.7463\n",
            "Epoch [2/10], Train Loss: 119.9461, Val Loss: 117.9491\n",
            "Epoch [3/10], Train Loss: 110.3271, Val Loss: 103.2280\n",
            "Epoch [4/10], Train Loss: 110.4028, Val Loss: 120.4531\n",
            "Epoch [5/10], Train Loss: 105.9169, Val Loss: 115.5938\n",
            "Epoch [6/10], Train Loss: 104.1790, Val Loss: 104.0525\n",
            "Epoch [7/10], Train Loss: 99.8783, Val Loss: 117.2026\n",
            "Epoch [8/10], Train Loss: 99.1729, Val Loss: 102.7946\n",
            "Epoch [9/10], Train Loss: 95.7003, Val Loss: 101.7430\n",
            "Epoch [10/10], Train Loss: 90.2155, Val Loss: 100.7431\n",
            "Training finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cba73ed1"
      },
      "source": [
        "## Next Steps for Improving Accuracy\n",
        "\n",
        "Based on the current results, here are some potential ways to further improve the model's accuracy:\n",
        "\n",
        "*   **Advanced Feature Engineering:** Explore more sophisticated techniques for extracting features from both tabular and image data. For tabular data, this could involve interaction terms, polynomial features, or dimensionality reduction. For image data, consider different image representations beyond simple bar plots (e.g., spectrograms generated from the raw voice signals if available) or using pre-trained CNNs as feature extractors.\n",
        "*   **More Complex Model Architectures:**\n",
        "    *   **Transformer Networks:** Explore using Transformer networks for both tabular and image data, or for fusing the features. Transformers are powerful for capturing long-range dependencies.\n",
        "    *   **Attention Mechanisms:** Incorporate attention mechanisms in the fusion layer to allow the model to dynamically weigh the importance of tabular and image features.\n",
        "    *   **Recurrent Neural Networks (RNNs) or LSTMs:** If the voice signal data has a temporal component within each recording, consider using RNNs or LSTMs to process the tabular data as a sequence.\n",
        "*   **Cross-Validation:** Implement k-fold cross-validation during training to get a more robust estimate of the model's performance and reduce the reliance on a single train-validation split.\n",
        "*   **Regularization Techniques:** Experiment with different regularization methods (e.g., L1, L2 regularization, more aggressive dropout) to prevent overfitting, especially if the model complexity is increased.\n",
        "*   **Ensemble Methods:** Train multiple different models (e.g., a model focusing on tabular data, a model focusing on image data, and the multimodal model) and combine their predictions.\n",
        "*   **Data Augmentation:** If possible, explore data augmentation techniques for the image data to increase the size and diversity of the training set.\n",
        "*   **Error Analysis:** Analyze the instances where the model makes large errors to understand if there are specific characteristics of the data that the model struggles with. This can inform further feature engineering or model modifications.\n",
        "*   **Experiment with Different Image Representations:** The current image representation is a simple bar plot of the features. Experiment with generating actual spectrograms or other image-based representations from the raw voice signal data if it's available.\n",
        "*   **Explore Different Loss Functions:** While MSE is standard for regression, other loss functions might be more appropriate depending on the specific characteristics of the target variable and the desired error profile."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d539da1"
      },
      "source": [
        "## Evaluate model\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the trained model on the validation set using appropriate regression metrics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e87152d7"
      },
      "source": [
        "**Reasoning**:\n",
        "Evaluate the model with the best hyperparameters on the validation set and calculate regression metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce3c29f6",
        "outputId": "062c6bab-d629-47cf-cc82-ef31679fd67f"
      },
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Choose the best performing hyperparameters (based on previous validation loss)\n",
        "# Experiment 1: {'lr': 1e-4, 'batch_size': 32, 'epochs': 15} had the lowest validation loss\n",
        "best_combo = {'lr': 1e-4, 'batch_size': 32, 'epochs': 15}\n",
        "\n",
        "# Re-initialize and train the model with the best hyperparameters\n",
        "# This step is needed as we need the trained model instance\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MultiModalNet(num_tab_features=len(FEATURE_COLS)).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=best_combo['lr'])\n",
        "\n",
        "# Create data loaders with the best batch size\n",
        "train_loader = DataLoader(train_dataset, batch_size=best_combo['batch_size'], shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=best_combo['batch_size'], shuffle=False)\n",
        "\n",
        "# Train the model again with the best hyperparameters\n",
        "print(f\"Training model with best hyperparameters: {best_combo}\")\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer, best_combo['epochs'], device)\n",
        "\n",
        "# --- Evaluation ---\n",
        "model.eval() # Set the model to evaluation mode\n",
        "\n",
        "all_predictions = []\n",
        "all_targets = []\n",
        "\n",
        "print(\"\\nEvaluating model on the validation set...\")\n",
        "with torch.no_grad(): # Disable gradient calculation\n",
        "    for tab_data, img, target in val_loader:\n",
        "        tab_data, img, target = tab_data.to(device), img.to(device), target.to(device)\n",
        "\n",
        "        outputs = model(tab_data, img).squeeze()\n",
        "\n",
        "        # Store predictions and targets\n",
        "        all_predictions.extend(outputs.cpu().numpy())\n",
        "        all_targets.extend(target.cpu().numpy())\n",
        "\n",
        "# Convert lists to numpy arrays for metric calculation\n",
        "all_predictions = np.array(all_predictions)\n",
        "all_targets = np.array(all_targets)\n",
        "\n",
        "# Calculate regression metrics\n",
        "mae = mean_absolute_error(all_targets, all_predictions)\n",
        "rmse = np.sqrt(mean_squared_error(all_targets, all_predictions))\n",
        "\n",
        "print(f\"\\nValidation MAE: {mae:.4f}\")\n",
        "print(f\"Validation RMSE: {rmse:.4f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model with best hyperparameters: {'lr': 0.0001, 'batch_size': 32, 'epochs': 15}\n",
            "Starting training for 15 epochs on cpu\n",
            "Epoch [1/15], Train Loss: 339.1626, Val Loss: 112.8125\n",
            "Epoch [2/15], Train Loss: 125.3157, Val Loss: 114.3209\n",
            "Epoch [3/15], Train Loss: 123.2566, Val Loss: 112.3860\n",
            "Epoch [4/15], Train Loss: 123.1786, Val Loss: 111.8404\n",
            "Epoch [5/15], Train Loss: 122.2056, Val Loss: 110.8395\n",
            "Epoch [6/15], Train Loss: 119.6952, Val Loss: 110.0828\n",
            "Epoch [7/15], Train Loss: 117.2115, Val Loss: 105.0811\n",
            "Epoch [8/15], Train Loss: 116.7090, Val Loss: 106.8743\n",
            "Epoch [9/15], Train Loss: 113.8609, Val Loss: 109.6360\n",
            "Epoch [10/15], Train Loss: 112.5993, Val Loss: 106.0228\n",
            "Epoch [11/15], Train Loss: 109.4934, Val Loss: 100.4519\n",
            "Epoch [12/15], Train Loss: 107.3539, Val Loss: 102.9285\n",
            "Epoch [13/15], Train Loss: 106.5878, Val Loss: 100.3033\n",
            "Epoch [14/15], Train Loss: 105.7413, Val Loss: 98.5814\n",
            "Epoch [15/15], Train Loss: 104.9033, Val Loss: 96.8472\n",
            "Training finished.\n",
            "\n",
            "Evaluating model on the validation set...\n",
            "\n",
            "Validation MAE: 7.9093\n",
            "Validation RMSE: 9.8411\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eece894"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The `ParkinsonDataset` class was successfully modified to load image paths from `updrs_image_paths.csv` and combine them with tabular voice signal data.\n",
        "*   The dataset was split into training (4700 samples) and validation (1175 samples) sets, and corresponding data loaders were created.\n",
        "*   An alternative fusion strategy was implemented in the `MultiModalNet`, adding more layers (Linear -> ReLU -> Dropout -> Linear -> ReLU) to the fusion part of the network to potentially allow for more complex feature interactions.\n",
        "*   A training loop with validation after each epoch was successfully implemented.\n",
        "*   Hyperparameter tuning experiments were conducted with three different combinations of learning rate, batch size, and epochs.\n",
        "*   The hyperparameter combination of learning rate 1e-4, batch size 32, and 15 epochs resulted in the lowest validation loss (approximately 92.7 during the tuning phase, and 96.85 during the final evaluation training).\n",
        "*   The final model evaluation on the validation set using the best hyperparameters yielded a Validation MAE of 7.9093 and a Validation RMSE of 9.8411.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The implemented multimodal approach using both tabular and image data shows promise, as indicated by the decreasing validation loss during training. Further investigation into the relative contributions of the tabular and image modalities could provide insights into their individual predictive power.\n",
        "*   The calculated MAE (\\$7.91\\$) and RMSE (\\$9.84\\$) provide quantitative measures of the model's performance in predicting UPDRS scores. These metrics can serve as a baseline for future model improvements or comparisons with other models.\n"
      ]
    }
  ]
}
